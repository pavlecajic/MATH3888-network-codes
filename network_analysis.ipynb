{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from community import community_louvain as cm\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read yeast protein interaction file\n",
    "g = nx.read_weighted_edgelist(\"4932.protein.links.v11.5.txt\",comments=\"#\",nodetype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholding\n",
    "threshold_score = 750\n",
    "for edge in g.edges: \n",
    "    weight = list(g.get_edge_data(edge[0],edge[1]).values())\n",
    "    if(weight[0] <= threshold_score):\n",
    "        g.remove_edge(edge[0],edge[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove essential nodes\n",
    "file = open(\"essential_nodes.txt\")\n",
    "for line in file.readlines():\n",
    "    node = line.strip()\n",
    "    if node in g:\n",
    "        g.remove_node(node)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select largest component\n",
    "components = sorted(nx.connected_components(g), key=len, reverse=True)\n",
    "g = g.subgraph(components[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new interactions\n",
    "g_mutant = g.copy()\n",
    "file = open(\"new_interactions.txt\")\n",
    "for line in file.readlines():\n",
    "    node = line.strip()\n",
    "    if node in g:\n",
    "        g_mutant.add_edge('4932.YJR104C', node)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_community(target,g):\n",
    "    global freq_dict\n",
    "    freq_dict={}\n",
    "    max=100\n",
    "    for node in list(g.nodes()):\n",
    "        freq_dict[node]=0\n",
    "    for i in range(0,max):\n",
    "        # louvain partition\n",
    "        partition = cm.best_partition(g)\n",
    "\n",
    "        target_module=partition[target]\n",
    "\n",
    "        for node in list(partition.keys()):\n",
    "            if partition[node]==target_module:\n",
    "                freq_dict[node]=freq_dict[node]+1\n",
    "    freq_dict_filt = dict((k, v) for k, v in freq_dict.items() if v > 0)\n",
    "    #Calculating size of module for each threshold\n",
    "    lengths=[]\n",
    "    for i in range(0,max):\n",
    "        freq_dict_filt = dict((k, v) for k, v in freq_dict.items() if v > i)\n",
    "        lengths.append(len(freq_dict_filt))\n",
    "    plt.plot(lengths)\n",
    "    plt.title('Frequency plot for nodes in {} community (100 trials)'.format(target))\n",
    "    plt.xlabel('Number of times appearing in {} community'.format(target))\n",
    "    plt.ylabel('Number of nodes')\n",
    "    plt.savefig(\"robust/{}.png\".format(target))\n",
    "    summary = [\"Node,Frequency\"]\n",
    "    for node in g:\n",
    "        summary.append(\"{},{}\".format(node, freq_dict[node]))\n",
    "    #write summary to file\n",
    "    with open(\"robust2/{}.csv\".format(target), \"w\") as file:\n",
    "        file.write(\"\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def participation_coefficient(graph, partition):\n",
    "    \"\"\"return a dictionary with {node:participation_coefficent}\"\"\"\n",
    "    #input: partition is a dictionary, {node:module}\n",
    "    pc_dict = {}\n",
    "    N = max(partition.values()) + 1\n",
    "    for v in graph:\n",
    "        k_is = [0]*N\n",
    "        for w in graph.neighbors(v):\n",
    "            k_is[partition[w]] += 1\n",
    "        pc_dict[v] = 1 - sum(i**2 for i in k_is)/ graph.degree(v)**2\n",
    "    return pc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_to_sod1(g):\n",
    "    \"\"\"return a dictionary giving the shortest path to SOD1\"\"\"\n",
    "    d = {}\n",
    "    for node in g:\n",
    "        d[node] = nx.shortest_path_length(g, '4932.YJR104C', node)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_matrix(g):\n",
    "    \"\"\"Return the subgraph centrality matrix\"\"\"\n",
    "    A = nx.to_numpy_array(g) # adjacency matrix\n",
    "    w, v = la.eig(A)\n",
    "    w=[l/max(w,key=abs) for l in w]\n",
    "    expdiag=[np.exp(l) for l in w]\n",
    "    intermediate=np.matmul(v,np.diag(expdiag))\n",
    "    subgraphmat=np.matmul(intermediate,np.linalg.inv(v))\n",
    "    subgraphmat=subgraphmat.real\n",
    "    return subgraphmat;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgraph_centralities(g):\n",
    "    \"\"\"Return both the regular and relative subgraph centralities\"\"\"\n",
    "    sc = {}\n",
    "    rsc = {}\n",
    "    A = sc_matrix(g)\n",
    "    i = 0\n",
    "    s = list(g.nodes()).index('4932.YJR104C') #index of sod1 (247 apparently)\n",
    "    for node in g:\n",
    "        sc[node] = A[i, i]\n",
    "        rsc[node] = A[i, s]\n",
    "        i += 1\n",
    "    return sc, rsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(g, filename):\n",
    "    \"\"\"Generate a summary of all the measures for each node in the graph g, and write to filename.csv\"\"\"\n",
    "    # louvain partition\n",
    "    partition = cm.best_partition(g, random_state=284)\n",
    "    # calculate participation coefficients\n",
    "    pc = participation_coefficient(g, partition)\n",
    "    # calculate proximites to SOD1\n",
    "    prox = proximity_to_sod1(g)\n",
    "    # calculate relative subgraph centralities\n",
    "    sc, rsc = subgraph_centralities(g)\n",
    "    # calculate closeness centrality\n",
    "    cc = nx.closeness_centrality(g)\n",
    "    # generate summary\n",
    "    summary = [\"Node,Community,Proximity to SOD1,Relative subgraph centrality,Subgraph centrality,Participation coefficient,Closeness Centrality\"]\n",
    "    for node in g:\n",
    "        summary.append(\"{},{},{},{},{},{},{}\".format(node, partition[node], prox[node], rsc[node], sc[node], pc[node], cc[node]))\n",
    "\n",
    "    #write summary to file\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(\"\\n\".join(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary(g, \"summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_summary(g_mutated, \"summary_mutated.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
